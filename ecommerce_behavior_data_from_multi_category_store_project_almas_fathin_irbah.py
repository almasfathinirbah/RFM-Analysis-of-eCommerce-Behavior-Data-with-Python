# -*- coding: utf-8 -*-
"""eCommerce Behavior Data from Multi Category Store Project - Almas Fathin Irbah.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17gqB60ZNKhr-m-oLAhb4qlRqqV228P6F

**Almas Fathin Irbah**
> Dataset : eCommerce behavior data from multi category store \
> Sumber : Kaggle \
> Link Dataset : https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store

# **Preparation**
"""

pip install squarify

# import library
import pandas as pd
import numpy as np
import seaborn as sns
import os
import squarify
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

# mounted google drive
import google.colab as gc
gc.drive.mount('/content/drive')

# check the path
!pwd

# Commented out IPython magic to ensure Python compatibility.
# change folder
# %cd '/content/drive/My Drive/data_file'

# check file in folder
!ls

"""# **OCTOBER 2019**

## **Handle The Missing Data**
"""

# read the data
cols = ['event_time', 'event_type', 'category_code', 'brand', 'price','user_id','user_session']
df2019oct = pd.read_csv('2019-Oct.csv', usecols = cols)
df2019oct.head(10)

df2019oct=df2019oct.loc[df2019oct.event_type == 'purchase']

# simple data checking - get row and column of dataframe
print(df2019oct.shape)

# simple data checking - get columns name
print(df2019oct.columns)

# simple data checking - get dataframe general information
df2019oct.info()

# check standard missing value - multiple column
missing_data = df2019oct.isnull()
for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("")

# check standard missing value - category_code column
df2019oct[df2019oct['category_code'].isnull()]

#check the most frequent value object 
df2019oct['category_code'].value_counts().idxmax()

# check standard missing value - brand column
df2019oct[df2019oct['brand'].isnull()]

# check the most frequent value object - brand column
df2019oct['brand'].value_counts().idxmax()

# handle missing data - replace values with the most frequent value object - category_code column
df2019oct['category_code'].replace(np.nan, 'electronics.smartphone', inplace=True)

# handle missing data - replace values with the most frequent value object- brand column
df2019oct['brand'].replace(np.nan, 'samsung', inplace=True)

# check again standard missing value - multiple column
missing_data = df2019oct.isnull()
for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("")

"""## **Handle Outlier**"""

# simple data checking - get dataframe description
df2019oct.describe()

df2019oct.head(10)

# assign variable for column in numeric type
numeric_column = ['price']

# check outlier using boxplot - 1
df2019oct.boxplot(
    column=['price'],
    fontsize=10,
    rot=0,
    grid=False,
    figsize=(5,5),
    vert=False
    )

# get IQR for each numeric column
Q1 = df2019oct[numeric_column].quantile(0.25)
Q3 = df2019oct[numeric_column].quantile(0.75)
IQR = Q3 - Q1
boxplot_min = Q1 - 1.5 * IQR
boxplot_max = Q3 + 1.5 * IQR
print('Q1:\n',Q1)
print('\nQ3:\n',Q3)
print('\nIQR:\n',IQR)
print('\nMin:\n',boxplot_min)
print('\nMax:\n',boxplot_max)

# filter price
filter_price_min = df2019oct['price']<boxplot_min['price']
filter_price_max = df2019oct['price']>boxplot_max['price']

df2019oct_non_outlier = df2019oct[(
    filter_price_min|filter_price_max
    )]

df2019oct_non_outlier.shape

df2019oct_non_outlier

"""## **Handle Date Column**"""

# simple data checking - get row and column of dataframe
print(df2019oct_non_outlier.shape)

# simple data checking - get columns name
print(df2019oct_non_outlier.columns)

# simple data checking - get dataframe general information
df2019oct_non_outlier.info()

# simple data checking - check 1 of the date column
df2019oct_non_outlier['event_time'].head(15)

df2019oct_non_outlier['event_time'].value_counts()

# change to date type & change date format
series_date_in_date = pd.to_datetime(df2019oct_non_outlier['event_time'],errors='raise',dayfirst=False,yearfirst=True,utc=True)
series_date_in_date

# parsing date column
series_date_in_date = pd.to_datetime(df2019oct_non_outlier['event_time'],errors='raise',dayfirst=False,yearfirst=True,utc=True)

series_date_in_date.dt.day

series_date_in_date.dt.month

series_date_in_date.dt.year

df2019oct_non_outlier['event_days_of_week'] = series_date_in_date.dt.day_name()
df2019oct_non_outlier['event_days_of_week']

df2019oct_non_outlier['event_date'] = series_date_in_date.dt.day
df2019oct_non_outlier['event_date']

df2019oct_non_outlier['event_hour'] = series_date_in_date.dt.hour
df2019oct_non_outlier['event_hour']

"""## **Best Products to Sell**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 

import matplotlib as mpl
import matplotlib.pyplot as plt

# Popular product in new user
new_user = df2019oct_non_outlier.drop_duplicates()
new_user['brand'].value_counts()

# Top 10 popular brands in new user
new_user['brand'].value_counts().nlargest(10).plot(
    kind='bar', 
    xlabel='Brands', 
    ylabel='Quantity', 
    label='Top 10 Popular Brands in New User', 
    subplots=True
    )

# Popular product in repeat user
repeat_user = df2019oct_non_outlier[df2019oct_non_outlier.duplicated()]
repeat_user['brand'].value_counts()

# Top 10 popular brands in repeat user
repeat_user['brand'].value_counts().nlargest(10).plot(
    kind='bar', 
    xlabel='Brands', 
    ylabel='Quantity', 
    label='Top 10 Popular Brands in Repeat User', 
    subplots=True
    )

# Popular categories in new user
new_user['category_code'].value_counts()

# Top 10 popular categories in new user
new_user['category_code'].value_counts().nlargest(10).plot(
    kind='bar', 
    xlabel='Categories', 
    ylabel='Quantity', 
    label='Top 10 Popular Categories in New User', 
    subplots=True
    )

# Popular categories in repeat user
repeat_user['category_code'].value_counts()

# Top 10 popular categories in repeat user
repeat_user['category_code'].value_counts().nlargest(10).plot(
    kind='bar', 
    xlabel='Categories', 
    ylabel='Quantity', 
    label='Top 10 Popular Categories in Repeat User', 
    subplots=True
    )

"""## **Best Event to predict that User most likely to buy a product**"""

# Best Event time
new_user['event_time'].value_counts().nlargest(10)

# Best Event time
repeat_user['event_time'].value_counts().nlargest(10)

# Best event type in new user
new_user['event_type'].value_counts()

# Best event type in repeat user
repeat_user['event_type'].value_counts()

"""## **Time Series**

### **New User**
"""

new_user.head()

ax = new_user.groupby(new_user['event_days_of_week'],sort=False)['user_id'].nunique().reset_index().set_index('event_days_of_week').plot.bar(figsize=(20,10))
ax.set_ylabel('# of all the transactions')
ax.set_xlabel('days of a week')
ax.set_title('Week days vs User')

for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 3,
            '{:1.2f}%'.format(height/len(new_user)*100),
            ha="center", fontsize=15)

plt.show()

#Timeplot01

new_user_timeline01 = new_user.groupby(new_user['event_date'])['user_id'].nunique().reset_index()
new_user_timeline01.columns = ['Event Date','User']
new_user_timeline01

x = np.arange(0,len(new_user_timeline01),1)

fig, ax = plt.subplots(1,1,figsize=(30,5))
ax.plot(x,new_user_timeline01['User'])
ax.set_xticks(x)
ax.set_xticklabels(new_user_timeline01['Event Date'])
ax.set_title('User in October 2019')
ax.set_xlabel('Date')
ax.set_ylabel('User')
  
plt.show()

#Timeplot02

new_user_timeline02 = new_user.groupby(new_user['event_hour'])['user_id'].nunique().reset_index()
new_user_timeline02.columns = ['Event Hour','User']
new_user_timeline02

x = np.arange(0,len(new_user_timeline02),1)

fig, ax = plt.subplots(1,1,figsize=(30,5))
ax.plot(x,new_user_timeline02['User'])
ax.set_xticks(x)
ax.set_xticklabels(new_user_timeline02['Event Hour'])
ax.set_title('User in 24 Hours')
ax.set_xlabel('Hour')
ax.set_ylabel('User')
    
plt.show()

"""### **Repeat User**"""

repeat_user.head()

ax = repeat_user.groupby(repeat_user['event_days_of_week'],sort=False)['user_id'].nunique().reset_index().set_index('event_days_of_week').plot.bar(figsize=(20,10))
ax.set_ylabel('# of all the transactions')
ax.set_xlabel('days of a week')
ax.set_title('Week days vs User')

plt.show()

#Timeplot01

repeat_user_timeline01 = repeat_user.groupby(repeat_user['event_date'])['user_id'].nunique().reset_index()
repeat_user_timeline01.columns = ['Event Date','User']
repeat_user_timeline01

x = np.arange(0,len(repeat_user_timeline01),1)

fig, ax = plt.subplots(1,1,figsize=(30,5))
ax.plot(x,repeat_user_timeline01['User'])
ax.set_xticks(x)
ax.set_xticklabels(repeat_user_timeline01['Event Date'])
ax.set_title('User in October 2019')
ax.set_xlabel('Date')
ax.set_ylabel('User')
  
plt.show()

#Timeplot02

repeat_user_timeline02 = repeat_user.groupby(repeat_user['event_hour'])['user_id'].nunique().reset_index()
repeat_user_timeline02.columns = ['Event Hour','User']
repeat_user_timeline02

x = np.arange(0,len(repeat_user_timeline02),1)

fig, ax = plt.subplots(1,1,figsize=(30,5))
ax.plot(x,repeat_user_timeline02['User'])
ax.set_xticks(x)
ax.set_xticklabels(repeat_user_timeline02['Event Hour'])
ax.set_title('User in 24 Hours')
ax.set_xlabel('Hour')
ax.set_ylabel('User')
    
plt.show()

"""# **NOVEMBER 2019**

## **Handle The Missing Data**
"""

# read the data
cols = ['event_time', 'event_type', 'category_code', 'brand', 'price','user_id','user_session']
df2019nov = pd.read_csv('2019-Nov.csv', usecols = cols)
df2019nov.head(10)

df2019nov=df2019nov.loc[df2019nov.event_type == 'purchase']

# simple data checking - get row and column of dataframe
print(df2019nov.shape)

# simple data checking - get columns name
print(df2019nov.columns)

# simple data checking - get dataframe general information
df2019nov.info()

# check standard missing value - multiple column
missing_data = df2019nov.isnull()
for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("")

# check standard missing value - category_code column
df2019nov[df2019nov['category_code'].isnull()]

#check the most frequent value object 
df2019nov['category_code'].value_counts().idxmax()

# check standard missing value - brand column
df2019nov[df2019nov['brand'].isnull()]

# check the most frequent value object - brand column
df2019nov['brand'].value_counts().idxmax()

# handle missing data - replace values with the most frequent value object - category_code column
df2019nov['category_code'].replace(np.nan, 'electronics.smartphone', inplace=True)

# handle missing data - replace values with the most frequent value object- brand column
df2019nov['brand'].replace(np.nan, 'samsung', inplace=True)

# check again standard missing value - multiple column
missing_data = df2019nov.isnull()
for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("")

"""## **Handle Outlier**"""

# simple data checking - get dataframe description
df2019nov.describe()

df2019nov.head(10)

# assign variable for column in numeric type
numeric_column = ['price']

# check outlier using boxplot - 1
df2019nov.boxplot(
    column=['price'],
    fontsize=10,
    rot=0,
    grid=False,
    figsize=(5,5),
    vert=False
    )

# get IQR for each numeric column
Q1 = df2019nov[numeric_column].quantile(0.25)
Q3 = df2019nov[numeric_column].quantile(0.75)
IQR = Q3 - Q1
boxplot_min = Q1 - 1.5 * IQR
boxplot_max = Q3 + 1.5 * IQR
print('Q1:\n',Q1)
print('\nQ3:\n',Q3)
print('\nIQR:\n',IQR)
print('\nMin:\n',boxplot_min)
print('\nMax:\n',boxplot_max)

# filter price
filter_price_min = df2019nov['price']<boxplot_min['price']
filter_price_max = df2019nov['price']>boxplot_max['price']

df2019nov_non_outlier = df2019nov[(
    filter_price_min|filter_price_max
    )]

df2019nov_non_outlier.shape

df2019nov_non_outlier.head(10)

"""## **Handle Date Column**"""

# simple data checking - get row and column of dataframe
print(df2019nov_non_outlier.shape)

# simple data checking - get columns name
print(df2019nov_non_outlier.columns)

# simple data checking - get dataframe general information
df2019nov_non_outlier.info()

# simple data checking - check 1 of the date column
df2019nov_non_outlier['event_time'].head(15)

df2019nov_non_outlier['event_time'].value_counts()

# change to date type & change date format
series_date_in_date = pd.to_datetime(df2019nov_non_outlier['event_time'],errors='raise',dayfirst=False,yearfirst=True,utc=True)
series_date_in_date

# parsing date column
series_date_in_date = pd.to_datetime(df2019nov_non_outlier['event_time'],errors='raise',dayfirst=False,yearfirst=True,utc=True)

series_date_in_date.dt.day

series_date_in_date.dt.month

series_date_in_date.dt.year

df2019nov_non_outlier['event_days_of_week'] = series_date_in_date.dt.day_name()
df2019nov_non_outlier['event_days_of_week']

df2019nov_non_outlier['event_date'] = series_date_in_date.dt.day
df2019nov_non_outlier['event_date']

df2019nov_non_outlier['event_hour'] = series_date_in_date.dt.hour
df2019nov_non_outlier['event_hour']

"""## **Best Products to Sell**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 

import matplotlib as mpl
import matplotlib.pyplot as plt

# Popular brands in new user
new_user = df2019nov_non_outlier.drop_duplicates()
new_user['brand'].value_counts()

# Top 10 popular brands in new user
new_user['brand'].value_counts().nlargest(10).plot(
    kind='bar', 
    xlabel='Brands', 
    ylabel='Quantity', 
    label='Top 10 Popular Brands in New User', 
    subplots=True
    )

# Popular product in repeat user
repeat_user = df2019nov_non_outlier[df2019nov_non_outlier.duplicated()]
repeat_user['brand'].value_counts()

# Popular categories in new user
new_user['category_code'].value_counts()

# Top 10 popular categories in new user
new_user['category_code'].value_counts().nlargest(10).plot(
    kind='bar', 
    xlabel='Categories', 
    ylabel='Quantity', 
    label='Top 10 Popular Categories in New User', 
    subplots=True
    )

# Popular categories in repeat user
repeat_user['category_code'].value_counts().nlargest(10)

"""## **Best Event to predict that User most likely to buy a product**"""

# Best Event time
new_user['event_time'].value_counts().nlargest(10)

# Best Event time
repeat_user['event_time'].value_counts().nlargest(10)

# Best event type in new user
new_user['event_type'].value_counts()

# Best event type in repeat user
repeat_user['event_type'].value_counts()

"""## **Time Series**

### **New User**
"""

new_user.head()

ax = new_user.groupby(new_user['event_days_of_week'],sort=False)['user_id'].nunique().reset_index().set_index('event_days_of_week').plot.bar(figsize=(20,10))
ax.set_ylabel('# of all the transactions')
ax.set_xlabel('days of a week')
ax.set_title('Week days vs all transaction')

for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 3,
            '{:1.2f}%'.format(height/len(new_user)*100),
            ha="center", fontsize=15)

plt.show()

#Timeplot01

new_user_timeline01 = new_user.groupby(new_user['event_date'])['user_id'].nunique().reset_index()
new_user_timeline01.columns = ['Event Date','User']
new_user_timeline01

x = np.arange(0,len(new_user_timeline01),1)

fig, ax = plt.subplots(1,1,figsize=(30,5))
ax.plot(x,new_user_timeline01['User'])
ax.set_xticks(x)
ax.set_xticklabels(new_user_timeline01['Event Date'])
ax.set_title('User in October 2019')
ax.set_xlabel('Date')
ax.set_ylabel('User')
  
plt.show()

#Timeplot02

new_user_timeline02 = new_user.groupby(new_user['event_hour'])['user_id'].nunique().reset_index()
new_user_timeline02.columns = ['Event Hour','User']
new_user_timeline02

x = np.arange(0,len(new_user_timeline02),1)

fig, ax = plt.subplots(1,1,figsize=(30,5))
ax.plot(x,new_user_timeline02['User'])
ax.set_xticks(x)
ax.set_xticklabels(new_user_timeline02['Event Hour'])
ax.set_title('User in 24 Hours')
ax.set_xlabel('Hour')
ax.set_ylabel('User')
    
plt.show()

"""### **Repeat User**"""

repeat_user.head()

"""# **RFM Analysis**

## **Preparation**
"""

frames = [df2019oct_non_outlier, df2019nov_non_outlier]
data=pd.concat(frames)

data.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data['event_time']=pd.to_datetime(data['event_time']).dt.tz_convert(None)

data.dtypes

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #save dataframe as feather in case our notebook got crashed
# #feather save column data types
# import pyarrow.feather as feather
# os.makedirs('tmp', exist_ok=True)  # Make a temp dir for storing the feather file
# feather.write_feather(data, './tmp/data')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #load the feather data cause feather more lightweight
# data = pd.read_feather('./tmp/data')
# data

data.dtypes

data=data.groupby(by='user_session').agg(Date_order=('event_time',lambda x: x.max()),
                                                  user_id=('user_id',lambda x: x.unique()),
                                          Quantity=('user_session','count'),
                                         money_spent=('price','sum')).reset_index(drop=True)
data

"""## **Modelling**"""

data['Date_order'].max()

import datetime as dt
study_date = dt.datetime(2019,12,1)
data=pd.DataFrame(data)
data['last_purchase']=study_date - data['Date_order']
data['last_purchase'].astype('timedelta64[D]')
data['last_purchase']=data['last_purchase'] / np.timedelta64(1, 'D')
data.head()

#Calculate Recency, Frequency, and Monetary of the data
RFM= data.groupby('user_id').agg(Recency=('last_purchase',lambda x: x.min()),
                                 Frequency=('user_id',lambda x: x.count()),
                                 Monetary=('money_spent',lambda x: x.sum()))
RFM.head()

"""**Frequency**"""

RFM['Frequency'].describe()

"""**Monetary**"""

RFM['Recency'].describe()

"""**RFM Segmentation**"""

RFM.quantile(q=[0.25,0.5,0.75])

quartiles=RFM.quantile(q=[0.25,0.5,0.75]).to_dict()
quartiles

"""Creation of RFM Segments"""

## for Recency 

def R(x,p,d):
    if x <= d[p][0.25]:
        return 1
    elif x <= d[p][0.50]:
        return 2
    elif x <= d[p][0.75]: 
        return 3
    else:
        return 4
    
## for Frequency and Monetary 

def FM(x,p,d):
    if x <= d[p][0.25]:
        return 4
    elif x <= d[p][0.50]:
        return 3
    elif x <= d[p][0.75]: 
        return 2
    else:
        return 1

#create RFM segmentation column
RFM['R_Quartile'] = RFM['Recency'].apply(R, args=('Recency',quartiles,))
RFM['F_Quartile'] = RFM['Frequency'].apply(FM, args=('Frequency',quartiles,))
RFM['M_Quartile'] = RFM['Monetary'].apply(FM, args=('Monetary',quartiles,))
RFM['RFM_segmentation'] = RFM.R_Quartile.map(str) \
                    + RFM.F_Quartile.map(str) \
                    + RFM.M_Quartile.map(str)
RFM['RFM_score'] = RFM.R_Quartile.map(int) \
                    + RFM.F_Quartile.map(int) \
                    + RFM.M_Quartile.map(int)
RFM.head()

# Define rfm_level function
def RFM_label(data):
    if data['RFM_score'] >= 10:
        return 'Lost'
    elif ((data['RFM_score'] >= 9) and (data['RFM_score'] < 10)):
        return 'Hibernating'
    elif ((data['RFM_score'] >= 8) and (data['RFM_score'] < 9)):
        return 'Can’t Lose Them'
    elif ((data['RFM_score'] >= 7) and (data['RFM_score'] < 8)):
        return 'About To Sleep'
    elif ((data['RFM_score'] >= 6) and (data['RFM_score'] < 7)):
        return 'Promising'
    elif ((data['RFM_score'] >= 5) and (data['RFM_score'] < 6)):
        return 'Potential Loyalist'
    elif ((data['RFM_score'] >= 4) and (data['RFM_score'] < 5)):
        return 'Loyal Customers'
    else:
        return 'Champions'
#Create RFM label for customer
RFM['RFM_label'] = RFM.apply(RFM_label, axis=1)
RFM.head()

# Calculate average values for each RFM_Level, and return a size of each segment 
RFM_desc = RFM.groupby('RFM_label').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': ['mean', 'count']
}).round(1)
# Print the aggregated dataset
print(RFM_desc)

#RFM_desc.columns = RFM_desc.columns.droplevel()
RFM_desc.columns = ['RecencyMean','FrequencyMean','MonetaryMean', 'Count']
#Create our plot and resize it.
fig = plt.gcf()
ax = fig.add_subplot()
fig.set_size_inches(16, 9)
squarify.plot(sizes=RFM_desc['Count'], 
              label=['Lost',
                     'Hibernating',
                     'Can’t Lose Them',
                     'About To Sleep',
                     'Promising', 
                     'Potential Loyalist', 
                     'Loyal Customers',
                     'Champions'], alpha=.6 )
plt.title("RFM Segments",fontsize=18,fontweight="bold")
plt.axis('off')
plt.show()